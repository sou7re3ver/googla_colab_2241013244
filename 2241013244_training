import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler  # MinMaxScaler optional

# ====================
# Load datasets
# ====================
try:
    train_df = pd.read_csv("/content/Train_data.csv")
    test_df = pd.read_csv("/content/Test_data.csv")
except FileNotFoundError:
    raise FileNotFoundError(
        "Make sure 'Train_data.csv' and 'Test_data.csv' are in '/content/' or provide correct paths."
    )

# ====================
# Dataset Overview
# ====================
print("Train Data Shape:", train_df.shape)
print("Test Data Shape:", test_df.shape)

print("\nTrain Data Info:")
print(train_df.info())

print("\nMissing values in Train Data:\n", train_df.isnull().sum())

print("\nTrain Data Describe:\n", train_df.describe(include="all"))

# ====================
# Label Encoding for categorical columns
# ====================
label_encoders = {}
categorical_cols = train_df.select_dtypes(include=['object']).columns

for col in categorical_cols:
    le = LabelEncoder()
    train_df[col] = le.fit_transform(train_df[col].astype(str))
    test_df[col] = le.transform(test_df[col].astype(str))
    label_encoders[col] = le

# ====================
# Feature Scaling
# ====================
target_col = 'target'  # Change this if your target column has a different name

if target_col not in train_df.columns:
    raise KeyError(f"Target column '{target_col}' not found in training data.")

numeric_cols = train_df.select_dtypes(include=[np.number]).columns.drop(target_col)

scaler = StandardScaler()
train_df[numeric_cols] = scaler.fit_transform(train_df[numeric_cols])
test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])

# ====================
# Target Variable Analysis
# ====================
plt.figure(figsize=(6,4))
sns.countplot(x=target_col, data=train_df)
plt.title("Target Variable Distribution")
plt.show()

# ====================
# Data Visualization
# ====================

# Boxplots for numeric features
for col in numeric_cols:
    plt.figure(figsize=(6,4))
    sns.boxplot(x=train_df[col])
    plt.title(f"Boxplot of {col}")
    plt.show()

# Bar plots for categorical variables
for col in categorical_cols:
    plt.figure(figsize=(6,4))
    counts = train_df[col].value_counts()
    sns.barplot(x=counts.index, y=counts.values)
    plt.title(f"Bar Plot of {col}")
    plt.xlabel(col)
    plt.ylabel("Count")
    plt.show()

import pandas as pd

train_path = "/content/Train_data.csv"
train_df = pd.read_csv(train_path)

# Show all column names
print("Column names in Train_data.csv:")
print(train_df.columns.tolist())

# Show first few rows
print("\nFirst 5 rows:")
print(train_df.head())



import matplotlib.pyplot as plt
import seaborn as sns

# Attack types across protocol types (categorical_col1)
plt.figure(figsize=(10,6))
sns.countplot(x='categorical_col1', hue='target', data=train_df)
plt.title("Target (Attack Types) Across Protocols (categorical_col1)")
plt.xlabel("Protocol Type (categorical_col1)")
plt.ylabel("Count")
plt.legend(title="Target (Attack Type)")
plt.show()

# Attack types across flags (categorical_col2)
plt.figure(figsize=(10,6))
sns.countplot(x='categorical_col2', hue='target', data=train_df)
plt.title("Target (Attack Types) Across Flags (categorical_col2)")
plt.xlabel("Flag (categorical_col2)")
plt.ylabel("Count")
plt.legend(title="Target (Attack Type)")
plt.show()


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# Copy dataset to avoid overwriting original
df_corr = train_df.copy()

# Encode categorical columns
categorical_cols = df_corr.select_dtypes(include=['object']).columns
for col in categorical_cols:
    le = LabelEncoder()
    df_corr[col] = le.fit_transform(df_corr[col].astype(str))

# Compute Pearson correlation
corr_matrix = df_corr.corr(method='pearson')

print("Pearson Correlation Matrix:")
print(corr_matrix)

# Plot heatmap
plt.figure(figsize=(8,6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Pearson Correlation Matrix Heatmap")
plt.show()

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE

# Load training data
train_path = "/content/Train_data.csv"
train_df = pd.read_csv(train_path)

# ====================
# Encode categorical columns
# ====================
categorical_cols = train_df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    le = LabelEncoder()
    train_df[col] = le.fit_transform(train_df[col].astype(str))

# ====================
# Separate features & target
# ====================
target_col = 'target'
X = train_df.drop(columns=[target_col])
y = train_df[target_col]

# ====================
# Apply RFE
# ====================
model = LogisticRegression(max_iter=1000, solver='liblinear')  # solver for small datasets
rfe = RFE(estimator=model, n_features_to_select=3)  # change number as needed
rfe.fit(X, y)

# ====================
# Results
# ====================
print("Selected Features:", X.columns[rfe.support_].tolist())
print("Feature Ranking:", dict(zip(X.columns, rfe.ranking_)))


import matplotlib.pyplot as plt

# Convert rankings to a DataFrame for plotting
ranking_df = pd.DataFrame({
    'Feature': X.columns,
    'Ranking': rfe.ranking_
}).sort_values('Ranking')

plt.figure(figsize=(6,4))
sns.barplot(x='Ranking', y='Feature', data=ranking_df, palette='viridis')
plt.title('RFE Feature Ranking (Lower = More Important)')
plt.xlabel('Ranking')
plt.ylabel('Feature')
plt.show()


import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
import xgboost as xgb

# ====================
# Load & Preprocess
# ====================
train_path = "/content/Train_data.csv"
train_df = pd.read_csv(train_path)

# Encode categorical columns
categorical_cols = train_df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    le = LabelEncoder()
    train_df[col] = le.fit_transform(train_df[col].astype(str))

# Split features & target
target_col = 'target'
X = train_df.drop(columns=[target_col])
y = train_df[target_col]

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# ====================
# Classifiers
# ====================
models = {
    "SVM": SVC(kernel='rbf', probability=True),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Logistic Regression": LogisticRegression(max_iter=1000, solver='liblinear'),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "KNN": KNeighborsClassifier(),
    "XGBoost": xgb.XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, random_state=42),
    "Naive Bayes": GaussianNB(),
    "AdaBoost": AdaBoostClassifier(random_state=42)
}

# ====================
# Training & Evaluation
# ====================
results = []
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results.append({"Model": name, "Accuracy": acc})
    print(f"=== {name} ===")
    print(f"Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred))
    print("\n")

# ====================
# Summary Table
# ====================
results_df = pd.DataFrame(results).sort_values(by="Accuracy", ascending=False)
print(results_df)
